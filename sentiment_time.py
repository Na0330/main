#!/usr/bin/env python
# coding: utf-8

# In[1]:


get_ipython().system('pip install WordCloud')
import re
import numpy as np
import pandas as pd
import seaborn as sns
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from nltk.stem import WordNetLemmatizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay


# In[2]:


# Importing the dataset

df = pd.read_csv('D:\\å¸ƒé‡Œæ–¯æ‰˜å¤§å­¦\\TB2\\EFIMM0139 - Social Media and Web Analytics\\Assessment\\data\\sentiment_time.csv')


# In[3]:


df.head()


# In[4]:


lab_to_sentiment = {0.0:"Negative", 1.0:"Neutral", 2.0:"Positive"}
def label_decoder(label):
  return lab_to_sentiment[label]
# å°†NaNå€¼æ›¿æ¢ä¸ºé»˜è®¤å€¼ï¼ˆä¾‹å¦‚ä¸­æ€§ï¼‰
df['sentiment'] = df['sentiment'].fillna(1.0)
df.sentiment = df.sentiment.apply(lambda x: label_decoder(x))
df.head()


# In[5]:


df.sentiment.value_counts()


# In[6]:


#Calculate the frequency distribution of emotional values
val_count = df.sentiment.value_counts()

# Draw a bar chart
plt.figure(figsize=(8,4))
plt.bar(val_count.index, val_count.values)

plt.title("Sentiment Data Distribution")
plt.xlabel("Sentiment")
plt.ylabel("Counts")


# In[7]:


negative_rows = df[df['sentiment'] == 'Negative']
negative_rows


# In[8]:


negative_ip_location = negative_rows.groupby(by=['note_ip_location']).size().reset_index(name='count').sort_values(by='count', ascending=False).head(5)
negative_ip_location


# In[9]:


negative_ip_location['note_ip_location'] = negative_ip_location['note_ip_location'].str.replace("\n",'')
negative_ip_location


# In[10]:


df['text'] = df['title'] + ' ' + df['desc']
df


# In[11]:


import nltk 
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.stem import SnowballStemmer


# In[12]:


# Remove punctuations
def remove_punctuations(data):
    return re.sub(r"[~.,ï¼Œ%/:;|?_&+*=!ï¼[ï¼šÃ—ï¼Ÿ#â€˜â€™ï¼›ï¼ˆï¼‰` â€¢ ã€‚...-~â€”]"," ",data)
df['text']= df['text'].apply(lambda x: remove_punctuations(x))


# In[13]:


import jieba
from nltk.stem import SnowballStemmer

# åˆ›å»º SnowballStemmer å¯¹è±¡
stemmer = SnowballStemmer('english')

# åˆ†è¯å¹¶è¿›è¡Œè¯å¹²æå–çš„å‡½æ•°
def stemming_on_text(text):
    # åˆ†è¯
    seg_list = jieba.cut(text, cut_all=False)
    # è¯å¹²æå–
    stemmed_text = " ".join([stemmer.stem(word) for word in seg_list])
    return stemmed_text

# å¯¹ 'text' åˆ—ä¸­çš„æ¯ä¸ªå¥å­åº”ç”¨åˆ†è¯å’Œè¯å¹²æå–å‡½æ•°
df['text'] = df['text'].apply(stemming_on_text)


# In[15]:


# Cleaning and removing the above stop words list from the little red book text
stop_words = stopwords.words('english')
new_stopwords = ['çš„', 'äº†','æ˜¯','R','æˆ‘','ç©¿','ä¼˜è¡£åº“','å¾ˆ','éƒ½','ä¹Ÿ','ä¹°','æœ‰','å°±','çœŸ','è¿˜',
                      'åœ¨','æ²¡','ä½†','ä»¬','ç‚¹','è¿™ä¸ª','è¿™','å»','å’Œ','ä¸','ä¸€','æ¬¾','å“­','æ„Ÿè§‰','ä¸Š',
                       'å•Š','ä¸€ä¸‹','è¦','å¤ª','ä»¶','ä¸ª','è¯•','ä¸‹','èƒ½','æ¡','åˆ','æ›´','è¯´','ç»™','å§','å¤§'
                 ,'èº«','ä¼š','æ¥','è°','è¿‡','å‡º','ä½ ','å•¦','åª','äºº','åˆ°','æƒ¹',']','r','ğŸ¤”','â¤'
                 ,'â­','ğŸ†˜','â“','â€¦','âœ…','â€¼','ğŸ¤©','âœ¨','ğŸ«¡','ğŸ”','å¦‚','å›¾','é¢˜','å“¦','å‘€','å—'
                ,'æ‰','æ—¥','åš','å†']
stop_words.extend(new_stopwords)
# Define function to remove Chinese emojis
def remove_chinese_emojis(text):
    emoji_pattern = re.compile("["
                               u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                               u"\U0001F680-\U0001F6FF"  # transport & map symbols
                               u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                               "]+", flags=re.UNICODE)
    return emoji_pattern.sub(r'', text)
def cleaning_stopwords(text):
    return " ".join([word for word in str(text).split() if word not in stop_words])
df['text'] = df['text'].apply(lambda text: cleaning_stopwords(text))
df['text'].head()


# In[16]:


# Cleaning and removing the nonsense words from the little red book text
import re

def remove_words(text, words_to_remove):
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ›¿æ¢è¦å»é™¤çš„å•è¯
    for word in words_to_remove:
        text = re.sub(r'\b' + re.escape(word) + r'\b', '', text)
    return text

# è¦å»é™¤çš„å•è¯åˆ—è¡¨
words_to_remove = ['çš„', 'äº†','æ˜¯','R','æˆ‘','ç©¿','ä¼˜è¡£åº“','å¾ˆ','éƒ½','ä¹Ÿ','ä¹°','æœ‰','å°±','çœŸ','è¿˜',
                      'åœ¨','æ²¡','ä½†','ä»¬','ç‚¹','è¿™ä¸ª','è¿™','å»','å’Œ','ä¸','ä¸€','æ¬¾','å“­','æ„Ÿè§‰','ä¸Š',
                       'å•Š','ä¸€ä¸‹','è¦','å¤ª','ä»¶','ä¸ª','è¯•','ä¸‹','èƒ½','æ¡','åˆ','æ›´','è¯´','ç»™','å§','å¤§'
                       ,'èº«','ä¼š','æ¥','è°','è¿‡','å‡º','ä½ ','å•¦','åª','äºº','åˆ°','æƒ¹','ä¹Ÿ','æˆ‘',']','r','ğŸ¤”','â¤'
                ,'â­','ğŸ†˜','â“','â€¦','âœ…','â€¼','ğŸ¤©','âœ¨','ğŸ«¡','â¤ï¸','ğŸ”','å¦‚','å›¾','é¢˜''å“¦','å‘€','å—'
                ,'æ‰','æ—¥','åš','å†']

# åœ¨ DataFrame çš„ text åˆ—ä¸Šåº”ç”¨å‡½æ•°
df['text'] = df['text'].apply(lambda x: remove_words(x, words_to_remove))
df['text'].head()


# In[17]:


# Remove numbers
def remove_numbers(data):
    return re.sub('[0-9]+', '', data)
df['text'] = df['text'].apply(lambda x: remove_numbers(x))


# In[18]:


df.head()


# In[19]:


new_df = df[['sentiment', 'text']].copy()
new_df


# In[20]:


# Positive wordcloud

from wordcloud import WordCloud

# Insert Chinese font
font_path = 'C:/Windows/Fonts/Deng.ttf'
plt.figure(figsize = (20,20)) 
wc = WordCloud(font_path=font_path, background_color = 'white',max_words = 2000 , width = 1600 , height = 800).generate(" ".join(df[df.sentiment == 'Positive'].text))
plt.imshow(wc , interpolation = 'bilinear')


# In[21]:


# Negative wordcloud
font_path = 'C:/Windows/Fonts/Deng.ttf'
plt.figure(figsize = (20,20)) 
wc = WordCloud(font_path=font_path, max_words = 2000 , width = 1600 , height = 800).generate(" ".join(df[df.sentiment == 'Negative'].text))
plt.imshow(wc , interpolation = 'bilinear')


# LDA Analysis

# In[24]:


import pandas as pd
import unicodedata
import re
import contractions
import string
get_ipython().system('pip install Gensim')
import gensim
import gensim.corpora as corpora
get_ipython().system('pip install spacy')
import spacy
get_ipython().system('pip install pyLDAvis')
import pyLDAvis
import pyLDAvis.gensim_models


# In[30]:


df_no_nans=new_df


# In[31]:


# Generating the document-term matrix and dictionary
def generate_tokens(tweet):
    words=[]
    for word in tweet.split(' '):
    # using the if condition because we introduced extra spaces during text cleaning
        if word!='':
           words.append(word)
    return words
#storing the generated tokens in a new column named 'tokens'
#df_no_nans['tokens']=df_no_nans.text.apply(generate_tokens)
df_no_nans.loc[:, 'tokens'] = df_no_nans['text'].apply(generate_tokens)


# In[32]:


df_no_nans['tokens'][0]


# In[33]:


def create_dictionary(words):
    return corpora.Dictionary(words)
id2word=create_dictionary(df_no_nans['tokens'])
print(id2word)


# In[34]:


def create_document_matrix(tokens,id2word):
    corpus = []
    for text in tokens:
        corpus.append(id2word.doc2bow(text))
    return corpus
#passing the dataframe column having tokens and dictionary
corpus=create_document_matrix(df_no_nans['tokens'],id2word)
print(df_no_nans['tokens'][0])
print(corpus[0])


# In[35]:


lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,
                                            id2word=id2word,
                                            num_topics=6,
                                            random_state=100,
                                             )


# In[36]:


pyLDAvis.enable_notebook()
vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word, mds="mmds", R=10)
vis


# In[37]:


pyLDAvis.save_json(vis, 'data.json')


# In[38]:


import json
import pandas as pd
import networkx as nx
import matplotlib.pyplot as plt


# In[39]:


# å‡è®¾pyLDAvisè¾“å‡ºçš„data.jsonæ–‡ä»¶åœ¨å½“å‰ç›®å½•
with open('data.json', 'r') as f:
    data = json.load(f)


# In[40]:


print(data.keys())


# In[41]:


import os
import math
import time
import nltk
from nltk.corpus import stopwords
from tqdm import tqdm
import re

from gensim.models import KeyedVectors
import gensim.downloader as api

import matplotlib.pyplot as plt
import seaborn as sns
import plotly.figure_factory as ff
import plotly.graph_objects as go
import plotly.express as px

# Below libraries are for text processing using NLTK
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer

# Below libraries are for feature representation using sklearn
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer

# Below libraries are for similarity matrices using sklearn
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics import pairwise_distances


# In[42]:


# Gets all the text for the desc column
desc_texts = new_df['text'].tolist()
print(desc_texts)


# In[43]:


# Output all text
for text in desc_texts:
    print(text)


# In[44]:


# åº”ç”¨é¢„å¤„ç†ï¼Œå¹¶åˆ†è¯ã€è¯å¹²æå–
from nltk.stem import PorterStemmer
nltk.download('punkt', quiet=True)
ps = PorterStemmer()


# In[45]:


from collections import Counter

# åˆ›å»ºä¸€ä¸ªç©ºçš„è®¡æ•°å™¨
word_counts = Counter()

# å¤„ç†æ¯ä¸ªå¤„ç†åçš„åˆ†è¯ç»“æœ
for processed_text in desc_texts:
    # å°†æ¯ä¸ªå¤„ç†åçš„åˆ†è¯ç»“æœæ‹†åˆ†æˆå•è¯åˆ—è¡¨
    words = processed_text.split()
    # æ›´æ–°è®¡æ•°å™¨
    word_counts.update(words)

# è¾“å‡ºæ¯ä¸ªå•è¯çš„å‡ºç°é¢‘ç‡
for word, count in word_counts.items():
    print(f"'{word}' , {count}")


# In[46]:


# è®¡ç®—æ€»è¯æ•°
total_word_count = sum(word_counts.values())
print("Total word count:", total_word_count)


# In[47]:


# è®¡ç®—æ–‡æ¡£é¢‘ç‡
document_frequency = Counter()

# å¤„ç†æ¯ä¸ªå¤„ç†åçš„åˆ†è¯ç»“æœ
for processed_text in desc_texts:
    # å°†æ¯ä¸ªå¤„ç†åçš„åˆ†è¯ç»“æœè½¬æ¢ä¸ºsetï¼Œä»¥å»é™¤é‡å¤è¯è¯­
    unique_words = set(processed_text.split())
    # æ›´æ–°æ–‡æ¡£é¢‘ç‡è®¡æ•°å™¨
    document_frequency.update(unique_words)

# è¾“å‡ºæ¯ä¸ªå•è¯çš„æ–‡æ¡£é¢‘ç‡
for word, freq in document_frequency.items():
    print(f"'{word}' , {freq}")


# In[48]:


import math

# è®¡ç®—æ¯ä¸ªå•è¯çš„TF-IDFå€¼
def calculate_tfidf(word_counts, total_word_count, document_frequency):
    tfidf_scores = {}
    num_documents = len(word_counts)

    for word, count in word_counts.items():
        # è®¡ç®—TFï¼ˆè¯é¢‘ï¼‰
        tf = count / total_word_count

        # è®¡ç®—IDFï¼ˆé€†æ–‡æ¡£é¢‘ç‡ï¼‰
        idf = math.log(num_documents / document_frequency[word])

        # è®¡ç®—TF-IDF
        tfidf_scores[word] = tf * idf

    return tfidf_scores

# å‡è®¾document_frequencyæ˜¯ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«æ¯ä¸ªå•è¯çš„æ–‡æ¡£é¢‘ç‡
# document_frequency[word]è¡¨ç¤ºåŒ…å«å•è¯wordçš„æ–‡æ¡£æ•°
tfidf_scores = calculate_tfidf(word_counts, total_word_count, document_frequency)
tfidf_scores


# In[128]:


# è·å–æœ€å¤§çš„TF-IDFå€¼
max_tfidf = max(tfidf_scores.values())

# å½’ä¸€åŒ–TF-IDFå€¼
normalized_tfidf_scores = {word: tfidf_score / max_tfidf for word, tfidf_score in tfidf_scores.items()}
normalized_tfidf_scores


# In[64]:


import networkx as nx
import matplotlib.pyplot as plt

plt.rcParams['font.sans-serif'] = ['SimHei']  # è®¾ç½®ä¸­æ–‡å­—ä½“ä¸ºé»‘ä½“
plt.rcParams['axes.unicode_minus'] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜

# æ ¹æ®é˜ˆå€¼æ„å»ºè¯­ä¹‰ç½‘ç»œåŒ–å›¾,é™åˆ¶èŠ‚ç‚¹æ•°é‡ä¸º100
def build_semantic_network(normalized_tfidf_scores, threshold=0.5, max_nodes=100):
    G = nx.Graph()

    # è·å–å‰100ä¸ªå•è¯åŠå…¶TF-IDFå¾—åˆ†
    top_words = sorted(normalized_tfidf_scores.items(), key=lambda x: x[1], reverse=True)[:max_nodes]

    # æ·»åŠ èŠ‚ç‚¹
    for word, score in top_words:
        G.add_node(word)
        
    # æ·»åŠ è¾¹
    for i, (word1, score1) in enumerate(top_words):
        for j, (word2, score2) in enumerate(top_words):
            if i != j:
                similarity = calculate_similarity(score1, score2)
                if similarity >= threshold:
                    G.add_edge(word1, word2, weight=similarity)

    return G

# è®¡ç®—å•è¯ä¹‹é—´çš„ç›¸ä¼¼åº¦
def calculate_similarity(score1, score2):
    # å‡è®¾ç›¸ä¼¼åº¦ç­‰äºä¸¤ä¸ªå¾—åˆ†çš„å·®çš„ç»å¯¹å€¼
    similarity = abs(score1 - score2)
    return similarity

# ä½¿ç”¨ TF-IDF å¾—åˆ†æ„å»ºè¯­ä¹‰ç½‘ç»œåŒ–å›¾ï¼Œé™åˆ¶èŠ‚ç‚¹æ•°é‡ä¸º100
semantic_network = build_semantic_network(normalized_tfidf_scores, max_nodes=100)

# ç»˜åˆ¶ç½‘ç»œå›¾
pos = nx.spring_layout(semantic_network, k=1)# kå‚æ•°æ§åˆ¶èŠ‚ç‚¹ä¹‹é—´çš„å¼¹ç°§åŠ›åº¦ï¼Œå¢åŠ æ­¤å€¼ä¼šå¢åŠ èŠ‚ç‚¹ä¹‹é—´çš„é—´è·
    
# ç»˜åˆ¶ç½‘ç»œå›¾
plt.figure(figsize=(10, 8), facecolor='white')
nx.draw(semantic_network, pos, node_color='skyblue', node_size=700, 
        edge_color='gray', with_labels=True)
plt.box(False)
plt.axis('off')
plt.show()


# In[67]:


import networkx as nx
import matplotlib.pyplot as plt

# å‡è®¾æ‚¨å·²ç»æœ‰äº†æ¯ä¸ªå•è¯çš„å‡ºç°é¢‘ç‡ï¼Œå­˜å‚¨åœ¨å­—å…¸ frequency_dict ä¸­ï¼Œå…¶ä¸­é”®æ˜¯å•è¯ï¼Œå€¼æ˜¯é¢‘ç‡
# ç¤ºä¾‹ï¼š
frequency_dict = word_counts

# æ ¹æ®é˜ˆå€¼æ„å»ºè¯­ä¹‰ç½‘ç»œåŒ–å›¾ï¼Œé™åˆ¶èŠ‚ç‚¹æ•°é‡ä¸º100
def build_semantic_network(normalized_tfidf_scores, frequency_dict, threshold=0.5, max_nodes=100):
    G = nx.Graph()

    # è·å–å‰100ä¸ªå•è¯åŠå…¶TF-IDFå¾—åˆ†
    top_words = sorted(normalized_tfidf_scores.items(), key=lambda x: x[1], reverse=True)[:max_nodes]

    # æ·»åŠ èŠ‚ç‚¹ï¼Œå¹¶ä¸ºæ¯ä¸ªèŠ‚ç‚¹æ·»åŠ é¢‘ç‡å±æ€§
    for word, score in top_words:
        G.add_node(word, frequency=frequency_dict.get(word, 0))  # ä½¿ç”¨getæ–¹æ³•è·å–é¢‘ç‡ï¼Œå¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œé»˜è®¤ä¸º0
        
    # æ·»åŠ è¾¹
    for i, (word1, score1) in enumerate(top_words):
        for j, (word2, score2) in enumerate(top_words):
            if i != j:
                similarity = calculate_similarity(score1, score2)
                if similarity >= threshold:
                    G.add_edge(word1, word2, weight=similarity)

    return G

# è®¡ç®—å•è¯ä¹‹é—´çš„ç›¸ä¼¼åº¦
def calculate_similarity(score1, score2):
    # å‡è®¾ç›¸ä¼¼åº¦ç­‰äºä¸¤ä¸ªå¾—åˆ†çš„å·®çš„ç»å¯¹å€¼
    similarity = abs(score1 - score2)
    return similarity

# ä½¿ç”¨ TF-IDF å¾—åˆ†æ„å»ºè¯­ä¹‰ç½‘ç»œåŒ–å›¾ï¼Œé™åˆ¶èŠ‚ç‚¹æ•°é‡ä¸º100
semantic_network = build_semantic_network(normalized_tfidf_scores, frequency_dict, max_nodes=100)

# ç»˜åˆ¶ç½‘ç»œå›¾
pos = nx.spring_layout(semantic_network, k=1.2)

# è·å–èŠ‚ç‚¹çš„é¢‘ç‡å±æ€§ï¼Œä½œä¸ºèŠ‚ç‚¹å¤§å°çš„ä¾æ®
node_sizes = [data['frequency'] for node, data in semantic_network.nodes(data=True)]

# ç»˜åˆ¶ç½‘ç»œå›¾ï¼Œè®¾ç½®èŠ‚ç‚¹å¤§å°ä¸ºäº¤äº’é¢‘ç‡çš„å¤§å°
plt.figure(figsize=(10, 8), facecolor='white')
nx.draw(semantic_network, pos, node_color='skyblue', node_size=node_sizes, 
        edge_color='gray', with_labels=True)
plt.box(False)
plt.axis('off')
plt.show()



# In[ ]:




